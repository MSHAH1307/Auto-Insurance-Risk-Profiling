{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\KUNAL MEHTA\\\\Desktop\\\\Data Science Training\\\\Projects\\\\Auto-Insurance-Risk-Profiling\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\KUNAL MEHTA\\\\Desktop\\\\Data Science Training\\\\Projects\\\\Auto-Insurance-Risk-Profiling'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/kunal1406/Auto-Insurance-Risk-Profiling.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"kunal1406\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"c1f8c1d6722f50e4980aec7e9eba0c1df1353ad6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClassModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    class_model_path: Path\n",
    "    all_params: dict\n",
    "    class_metric_file_name: Path\n",
    "    mlflow_uri: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RegModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    reg_model_path: Path\n",
    "    all_params: dict\n",
    "    reg_metric_file_name: Path\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoInsurance.constants import *\n",
    "from AutoInsurance.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_class_model_evaluation_config(self) -> ClassModelEvaluationConfig:\n",
    "        config = self.config.class_model_evaluation\n",
    "        params = self.params.GradientBoostingClassifier\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        class_model_evaluation_config = ClassModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_class_path,\n",
    "            test_data_path= config.test_data_path,\n",
    "            class_model_path = config.class_model_path,\n",
    "            all_params= params,\n",
    "            class_metric_file_name = config.class_metric_file_name,\n",
    "            mlflow_uri = \"https://dagshub.com/kunal1406/Auto-Insurance-Risk-Profiling.mlflow\"\n",
    "        )\n",
    "\n",
    "        return class_model_evaluation_config\n",
    "    \n",
    "    def get_reg_model_evaluation_config(self) -> RegModelEvaluationConfig:\n",
    "        config = self.config.reg_model_evaluation\n",
    "        params = self.params.GradientBoostingRegressor\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        reg_model_evaluation_config = RegModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_reg_path,\n",
    "            test_data_path= config.test_data_path,\n",
    "            reg_model_path = config.reg_model_path,\n",
    "            all_params= params,\n",
    "            reg_metric_file_name = config.reg_metric_file_name,\n",
    "            mlflow_uri = \"https://dagshub.com/kunal1406/Auto-Insurance-Risk-Profiling.mlflow\"\n",
    "        )\n",
    "\n",
    "        return reg_model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error, r2_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelEvaluation:\n",
    "    def __init__(self, config: ClassModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def perform_k_fold(self, X, y):\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=45)\n",
    "        cv_scores = []\n",
    "        pred_full = np.zeros(y.shape[0]) \n",
    "        true_full = np.zeros(y.shape[0]) \n",
    "\n",
    "        i = 1\n",
    "\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            print(f\"Fold {i} started of {kf.n_splits}\")\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            gb = GradientBoostingClassifier(learning_rate=0.1,max_depth=4,max_features=0.3,min_samples_leaf=5,n_estimators=100)\n",
    "            gb.fit(X_train, y_train)\n",
    "            pred_probs = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            pred_full[test_index] = pred_probs  \n",
    "            true_full[test_index] = y_test  \n",
    "\n",
    "            score = roc_auc_score(y_test, pred_probs)\n",
    "            print('roc_auc_score', score)\n",
    "            cv_scores.append(score)\n",
    "\n",
    "            i += 1\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(true_full, pred_full)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        print(\"optimal threshold is\", optimal_threshold)\n",
    "\n",
    "        predicted_labels = (pred_full >= optimal_threshold)\n",
    "        report = classification_report(true_full, predicted_labels, output_dict=True)\n",
    "        print(report)\n",
    "\n",
    "        return cv_scores, optimal_threshold, report\n",
    "\n",
    "    def evaluate_model(self, X, y):\n",
    "        cv_scores, optimal_threshold, report = self.perform_k_fold(X, y)\n",
    "        mean_score = np.mean(cv_scores)\n",
    "        std_score = np.std(cv_scores)\n",
    "        print(f\"Mean roc_auc_score: {mean_score}\")\n",
    "        print(f\"Std roc_auc_score: {std_score}\")\n",
    "        return mean_score, std_score, optimal_threshold, report\n",
    "    def log_into_mlflow(self):\n",
    "\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "\n",
    "            data = pd.read_csv(self.config.train_data_path)\n",
    "            X = data.drop('claim', axis=1)\n",
    "            print(X.shape)\n",
    "            y = data['claim']\n",
    "            print(y.shape)\n",
    "            roc_auc_score, std_roc_auc_score, optimal_threshold, report = self.evaluate_model(X, y)\n",
    "\n",
    "            scores = {\"roc_auc_score\": roc_auc_score, \"optimal_threshold\": optimal_threshold}\n",
    "            save_json(path=Path(self.config.class_metric_file_name), data=scores)\n",
    "\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metric(\"roc_auc_score\", roc_auc_score)\n",
    "            mlflow.log_metric(\"std roc_auc_score\", std_roc_auc_score)\n",
    "            mlflow.log_metric(\"optimal_threshold\", optimal_threshold)\n",
    "\n",
    "            for label, metric in report.items():\n",
    "                if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                    for metric_name, metric_value in metric.items():\n",
    "                        mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegModelEvaluation:\n",
    "    def __init__(self, config: RegModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def perform_k_fold(self, X, y):\n",
    "        model = GradientBoostingRegressor(\n",
    "            learning_rate=0.1,\n",
    "            max_depth=4,\n",
    "            max_features=0.3,\n",
    "            min_samples_leaf=5,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=45)\n",
    "        cv_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "        return model, cv_scores\n",
    "    \n",
    "    def evaluate_model(self, model, X_train, y_train, X_test, y_test):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "        return r2, rmse, mae, predictions\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            data = pd.read_csv(self.config.train_data_path)\n",
    "            X = data.drop('log_amount', axis=1)\n",
    "            y = data['log_amount']\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "\n",
    "            model, cv_scores = self.perform_k_fold(X, y)\n",
    "\n",
    "            r2, rmse, mae, predictions = self.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "            scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "            save_json(path=Path(self.config.reg_metric_file_name), data=scores)\n",
    "\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metric(\"mean_cv_r2_score\", np.mean(cv_scores))\n",
    "            mlflow.log_metric(\"std_cv_r2_score\", np.std(cv_scores))\n",
    "            mlflow.log_metric(\"r2_score\", r2)\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "            mlflow.log_metric(\"mae\", mae)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 23:58:02,320: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-22 23:58:02,325: INFO: common: yaml file: params.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-22 23:58:02,330: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-05-22 23:58:02,332: INFO: common: created directory at: artifacts]\n",
      "[2024-05-22 23:58:02,334: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "(60392, 29)\n",
      "(60392,)\n",
      "Fold 1 started of 10\n",
      "roc_auc_score 0.8161758520349216\n",
      "Fold 2 started of 10\n",
      "roc_auc_score 0.8270359657576805\n",
      "Fold 3 started of 10\n",
      "roc_auc_score 0.8319984446976783\n",
      "Fold 4 started of 10\n",
      "roc_auc_score 0.8331943209291902\n",
      "Fold 5 started of 10\n",
      "roc_auc_score 0.8089516201197836\n",
      "Fold 6 started of 10\n",
      "roc_auc_score 0.8237128368666835\n",
      "Fold 7 started of 10\n",
      "roc_auc_score 0.8115629283713593\n",
      "Fold 8 started of 10\n",
      "roc_auc_score 0.8232392774021067\n",
      "Fold 9 started of 10\n",
      "roc_auc_score 0.8287553938660587\n",
      "Fold 10 started of 10\n",
      "roc_auc_score 0.8128729379771725\n",
      "optimal threshold is 0.15642630430428492\n",
      "{'0.0': {'precision': 0.9361289997987523, 'recall': 0.7389102895039911, 'f1-score': 0.8259094036442728, 'support': 50362.0}, '1.0': {'precision': 0.3629360465116279, 'recall': 0.7468594217347956, 'f1-score': 0.4884903814802739, 'support': 10030.0}, 'accuracy': 0.7402304941051795, 'macro avg': {'precision': 0.6495325231551901, 'recall': 0.7428848556193933, 'f1-score': 0.6571998925622733, 'support': 60392.0}, 'weighted avg': {'precision': 0.8409321968866139, 'recall': 0.7402304941051795, 'f1-score': 0.769870312501325, 'support': 60392.0}}\n",
      "Mean roc_auc_score: 0.8217499578022635\n",
      "Std roc_auc_score: 0.008345891592925113\n",
      "[2024-05-22 23:58:52,422: INFO: common: created directory at: artifacts/model_evaluation]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "\n",
    "class_model_evaluation_config = config.get_class_model_evaluation_config()\n",
    "class_model_evaluation_config = ClassModelEvaluation(config=class_model_evaluation_config)\n",
    "class_model_evaluation_config.log_into_mlflow()\n",
    "\n",
    "reg_model_evaluation_config = config.get_reg_model_evaluation_config()\n",
    "reg_model_evaluation_config = RegModelEvaluation(config=reg_model_evaluation_config)\n",
    "reg_model_evaluation_config.log_into_mlflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
