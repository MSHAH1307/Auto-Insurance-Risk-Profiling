{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\KUNAL MEHTA\\\\Desktop\\\\Data Science Training\\\\Projects\\\\Auto-Insurance-Risk-Profiling\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\KUNAL MEHTA\\\\Desktop\\\\Data Science Training\\\\Projects\\\\Auto-Insurance-Risk-Profiling'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('artifacts\\data_transformation\\Processed_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['credit_score', 'traffic_index', 'veh_value', 'gender_M', 'area_B',\n",
       "       'area_C', 'area_D', 'area_E', 'area_F', 'veh_body_CONVT',\n",
       "       'veh_body_COUPE', 'veh_body_HBACK', 'veh_body_HDTOP', 'veh_body_MCARA',\n",
       "       'veh_body_MIBUS', 'veh_body_PANVN', 'veh_body_RDSTR', 'veh_body_SEDAN',\n",
       "       'veh_body_STNWG', 'veh_body_TRUCK', 'veh_body_UTE', 'agecat_2',\n",
       "       'agecat_3', 'agecat_4', 'agecat_5', 'agecat_6', 'veh_age_2',\n",
       "       'veh_age_3', 'veh_age_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_score      float64\n",
       "traffic_index     float64\n",
       "veh_value         float64\n",
       "gender_M             bool\n",
       "area_B               bool\n",
       "area_C               bool\n",
       "area_D               bool\n",
       "area_E               bool\n",
       "area_F               bool\n",
       "veh_body_CONVT       bool\n",
       "veh_body_COUPE       bool\n",
       "veh_body_HBACK       bool\n",
       "veh_body_HDTOP       bool\n",
       "veh_body_MCARA       bool\n",
       "veh_body_MIBUS       bool\n",
       "veh_body_PANVN       bool\n",
       "veh_body_RDSTR       bool\n",
       "veh_body_SEDAN       bool\n",
       "veh_body_STNWG       bool\n",
       "veh_body_TRUCK       bool\n",
       "veh_body_UTE         bool\n",
       "agecat_2             bool\n",
       "agecat_3             bool\n",
       "agecat_4             bool\n",
       "agecat_5             bool\n",
       "agecat_6             bool\n",
       "veh_age_2            bool\n",
       "veh_age_3            bool\n",
       "veh_age_4            bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   credit_score  traffic_index  veh_value  gender_M  area_B  area_C  area_D  \\\n",
      "0           500            1.9        1.5     False   False   False   False   \n",
      "\n",
      "   area_E  area_F  veh_body_CONVT  ...  veh_body_TRUCK  veh_body_UTE  \\\n",
      "0   False   False           False  ...           False         False   \n",
      "\n",
      "   agecat_2  agecat_3  agecat_4  agecat_5  agecat_6  veh_age_2  veh_age_3  \\\n",
      "0     False      True     False     False     False      False      False   \n",
      "\n",
      "   veh_age_4  \n",
      "0      False  \n",
      "\n",
      "[1 rows x 29 columns]\n",
      "credit_score        int64\n",
      "traffic_index     float64\n",
      "veh_value         float64\n",
      "gender_M             bool\n",
      "area_B               bool\n",
      "area_C               bool\n",
      "area_D               bool\n",
      "area_E               bool\n",
      "area_F               bool\n",
      "veh_body_CONVT       bool\n",
      "veh_body_COUPE       bool\n",
      "veh_body_HBACK       bool\n",
      "veh_body_HDTOP       bool\n",
      "veh_body_MCARA       bool\n",
      "veh_body_MIBUS       bool\n",
      "veh_body_PANVN       bool\n",
      "veh_body_RDSTR       bool\n",
      "veh_body_SEDAN       bool\n",
      "veh_body_STNWG       bool\n",
      "veh_body_TRUCK       bool\n",
      "veh_body_UTE         bool\n",
      "agecat_2             bool\n",
      "agecat_3             bool\n",
      "agecat_4             bool\n",
      "agecat_5             bool\n",
      "agecat_6             bool\n",
      "veh_age_2            bool\n",
      "veh_age_3            bool\n",
      "veh_age_4            bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the function to transform user data into the DataFrame\n",
    "def transform_user_data_to_df(user_data, columns, dtypes):\n",
    "    # Initialize the DataFrame with zeros and the specified data types\n",
    "    data = {col: np.zeros(1, dtype=dt) if dt == 'float64' else np.zeros(1, dtype=bool) for col, dt in dtypes.items()}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Fill in the continuous/numerical data\n",
    "    if 'credit_score' in user_data:\n",
    "        df['credit_score'] = user_data['credit_score']\n",
    "    if 'traffic_index' in user_data:\n",
    "        df['traffic_index'] = user_data['traffic_index']\n",
    "    if 'veh_value' in user_data:\n",
    "        df['veh_value'] = user_data['veh_value']\n",
    "    \n",
    "    # Handle categorical data by setting the relevant column to True\n",
    "    for key, value in user_data.items():\n",
    "        if key in ['gender', 'area', 'veh_body', 'agecat', 'veh_age']:\n",
    "            column_name = f'{key}_{value}'\n",
    "            if column_name in df.columns:\n",
    "                df[column_name] = True\n",
    "    \n",
    "    # Return the resulting DataFrame\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "user_data = {\n",
    "    'gender': 'F',\n",
    "    'agecat': '3',\n",
    "    'credit_score': 500,\n",
    "    'area': 'A',\n",
    "    'traffic_index': 1.9,\n",
    "    'veh_age': '1',\n",
    "    'veh_body': 'SEDAN',\n",
    "    'veh_value': 1.5\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame with the specified columns and data types\n",
    "columns = test.columns\n",
    "dtypes = test.dtypes\n",
    "\n",
    "# Transform the user data into the DataFrame\n",
    "df = transform_user_data_to_df(user_data, columns, dtypes)\n",
    "print(df)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_model = joblib.load(Path(\"artifacts\\model_trainer\\class_model.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = joblib.load(Path('artifacts/model_trainer/reg_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predictions_probs = class_model.predict_proba(df)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07961550057185947"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_predictions_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6434.927335660211"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_predictions = reg_model.predict(df)\n",
    "reg_predictions = np.expm1(reg_predictions)\n",
    "reg_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_likelihood = class_predictions_probs[0]\n",
    "claim_amount = reg_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load(Path('artifacts/risk_profiles/minmax_scaler.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.read_csv('artifacts/risk_profiles/risk_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL MEHTA\\Desktop\\Data Science Training\\Projects\\Auto-Insurance-Risk-Profiling\\venv\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "norm_values = scaler.transform([[claim_likelihood, claim_amount]])\n",
    "normalized_claim_likelihood = norm_values[0, 0]\n",
    "normalized_claim_amount = norm_values[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['quote_number', 'gender', 'agecat', 'date_of_birth', 'credit_score',\n",
       "       'area', 'traffic_index', 'veh_age', 'veh_body', 'veh_value', 'age',\n",
       "       'claim_probability', 'claim', 'claim_amount',\n",
       "       'normalized_claim_probability', 'normalized_claim_amount',\n",
       "       'risk_profile_probability', 'risk_profile_cost',\n",
       "       'weighted_probability_score', 'weighted_cost_score',\n",
       "       'dynamic_combined_risk_score', 'risk_group'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_probability_thresholds = [0.2, 0.45]\n",
    "claim_amount_thresholds = [0.2, 0.45]\n",
    "weights_probability = {'Low': 0.4, 'Medium': 0.5, 'High': 0.6}\n",
    "weights_cost = {'No Claim': 0.3, 'Low': 0.4, 'Medium': 0.5, 'High': 0.6}\n",
    "risk_score_thresholds = [0.2, 0.45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_prob = predictions_df['claim_probability'].quantile(claim_probability_thresholds)\n",
    "if normalized_claim_likelihood <= quantiles_prob.iloc[0]:\n",
    "    risk_profile_probability = 'Low'\n",
    "elif normalized_claim_likelihood <= quantiles_prob.iloc[1]:\n",
    "    risk_profile_probability = 'Medium'\n",
    "else:\n",
    "    risk_profile_probability = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "if claim_amount == 0:\n",
    "    risk_profile_cost = 'No Claim'\n",
    "else:\n",
    "    quantiles_cost = predictions_df.loc[predictions_df['claim_amount'] > 0, 'claim_amount'].quantile(claim_amount_thresholds)\n",
    "    if normalized_claim_amount <= quantiles_cost.iloc[0]:\n",
    "        risk_profile_cost = 'Low'\n",
    "    elif normalized_claim_amount <= quantiles_cost.iloc[1]:\n",
    "        risk_profile_cost = 'Medium'\n",
    "    else:\n",
    "        risk_profile_cost = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_probability_score = normalized_claim_likelihood * weights_probability[risk_profile_probability]\n",
    "weighted_cost_score = normalized_claim_amount * weights_cost[risk_profile_cost]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_combined_risk_score = weighted_probability_score + weighted_cost_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim_likelihood': 0.07961550057185947,\n",
       " 'claim_amount': 6434.927335660211,\n",
       " 'normalized_claim_likelihood': 0.07556854055743428,\n",
       " 'normalized_claim_amount': 0.19096796236790398,\n",
       " 'risk_profile_probability': 'Medium',\n",
       " 'risk_profile_cost': 'Low',\n",
       " 'dynamic_combined_risk_score': 0.11417145522587874,\n",
       " 'risk_group': 'High Risk'}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles_risk = predictions_df['dynamic_combined_risk_score'].quantile(risk_score_thresholds)\n",
    "if dynamic_combined_risk_score <= quantiles_risk.iloc[0]:\n",
    "    risk_group = 'Low Risk'\n",
    "elif dynamic_combined_risk_score <= quantiles_risk.iloc[1]:\n",
    "    risk_group = 'Medium Risk'\n",
    "else:\n",
    "    risk_group = 'High Risk'\n",
    "{\n",
    "    'claim_likelihood': claim_likelihood,\n",
    "    'claim_amount': claim_amount,\n",
    "    'normalized_claim_likelihood': normalized_claim_likelihood,\n",
    "    'normalized_claim_amount': normalized_claim_amount,\n",
    "    'risk_profile_probability': risk_profile_probability,\n",
    "    'risk_profile_cost': risk_profile_cost,\n",
    "    'dynamic_combined_risk_score': dynamic_combined_risk_score,\n",
    "    'risk_group': risk_group\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim_likelihood': 0.029240127757896677, 'claim_amount': 542.7766554252737, 'normalized_claim_likelihood': 0.018825345723338945, 'normalized_claim_amount': 0.01335627955937886, 'risk_profile_probability': 'Low', 'risk_profile_cost': 'Low', 'dynamic_combined_risk_score': 0.012872650113087122, 'risk_group': 'Low Risk'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL MEHTA\\Desktop\\Data Science Training\\Projects\\Auto-Insurance-Risk-Profiling\\venv\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "\n",
    "class RiskProfileModel:\n",
    "    def __init__(self, model_paths, scaler_path, risk_profile_path):\n",
    "        self.class_model = joblib.load(Path(model_paths['class_model']))\n",
    "        self.reg_model = joblib.load(Path(model_paths['reg_model']))\n",
    "        self.scaler = joblib.load(Path(scaler_path))\n",
    "        self.risk_profiles_df = pd.read_csv(risk_profile_path)\n",
    "    \n",
    "    def transform_user_data_to_df(self, user_data, columns, dtypes):\n",
    "        data = {col: np.zeros(1, dtype=dt) if dt == 'float64' else np.zeros(1, dtype=bool) for col, dt in dtypes.items()}\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if 'credit_score' in user_data:\n",
    "            df['credit_score'] = user_data['credit_score']\n",
    "        if 'traffic_index' in user_data:\n",
    "            df['traffic_index'] = user_data['traffic_index']\n",
    "        if 'veh_value' in user_data:\n",
    "            df['veh_value'] = user_data['veh_value']\n",
    "        \n",
    "        for key, value in user_data.items():\n",
    "            if key in ['gender', 'area', 'veh_body', 'agecat', 'veh_age']:\n",
    "                column_name = f'{key}_{value}'\n",
    "                if column_name in df.columns:\n",
    "                    df[column_name] = True\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def predict(self, user_data, columns, dtypes):\n",
    "        df = self.transform_user_data_to_df(user_data, columns, dtypes)\n",
    "        class_predictions_probs = self.class_model.predict_proba(df)[:, 1]\n",
    "        reg_predictions = self.reg_model.predict(df)\n",
    "        reg_predictions = np.expm1(reg_predictions)\n",
    "        \n",
    "        claim_likelihood = class_predictions_probs[0]\n",
    "        claim_amount = reg_predictions[0]\n",
    "        \n",
    "        return claim_likelihood, claim_amount\n",
    "    \n",
    "    def normalize_predictions(self, claim_likelihood, claim_amount):\n",
    "        features = pd.DataFrame([[claim_likelihood, claim_amount]], columns=['claim_probability', 'claim_amount'])\n",
    "\n",
    "        norm_values = self.scaler.transform(features)\n",
    "        # norm_values = self.scaler.transform([[claim_likelihood, claim_amount]])\n",
    "        normalized_claim_likelihood = norm_values[0, 0]\n",
    "        normalized_claim_amount = norm_values[0, 1]\n",
    "        \n",
    "        return normalized_claim_likelihood, normalized_claim_amount\n",
    "    \n",
    "    def classify_risk(self, normalized_claim_likelihood, normalized_claim_amount, claim_amount):\n",
    "        claim_probability_thresholds = [0.2, 0.45]\n",
    "        claim_amount_thresholds = [0.2, 0.45]\n",
    "        weights_probability = {'Low': 0.4, 'Medium': 0.5, 'High': 0.6}\n",
    "        weights_cost = {'No Claim': 0.3, 'Low': 0.4, 'Medium': 0.5, 'High': 0.6}\n",
    "        risk_score_thresholds = [0.2, 0.45]\n",
    "        \n",
    "        quantiles_prob = self.risk_profiles_df['claim_probability'].quantile(claim_probability_thresholds)\n",
    "        if normalized_claim_likelihood <= quantiles_prob.iloc[0]:\n",
    "            risk_profile_probability = 'Low'\n",
    "        elif normalized_claim_likelihood <= quantiles_prob.iloc[1]:\n",
    "            risk_profile_probability = 'Medium'\n",
    "        else:\n",
    "            risk_profile_probability = 'High'\n",
    "        \n",
    "        if claim_amount == 0:\n",
    "            risk_profile_cost = 'No Claim'\n",
    "        else:\n",
    "            quantiles_cost = self.risk_profiles_df.loc[self.risk_profiles_df['claim_amount'] > 0, 'claim_amount'].quantile(claim_amount_thresholds)\n",
    "            if normalized_claim_amount <= quantiles_cost.iloc[0]:\n",
    "                risk_profile_cost = 'Low'\n",
    "            elif normalized_claim_amount <= quantiles_cost.iloc[1]:\n",
    "                risk_profile_cost = 'Medium'\n",
    "            else:\n",
    "                risk_profile_cost = 'High'\n",
    "        \n",
    "        weighted_probability_score = normalized_claim_likelihood * weights_probability[risk_profile_probability]\n",
    "        weighted_cost_score = normalized_claim_amount * weights_cost[risk_profile_cost]\n",
    "        dynamic_combined_risk_score = weighted_probability_score + weighted_cost_score\n",
    "        \n",
    "        quantiles_risk = self.risk_profiles_df['dynamic_combined_risk_score'].quantile(risk_score_thresholds)\n",
    "        if dynamic_combined_risk_score <= quantiles_risk.iloc[0]:\n",
    "            risk_group = 'Low Risk'\n",
    "        elif dynamic_combined_risk_score <= quantiles_risk.iloc[1]:\n",
    "            risk_group = 'Medium Risk'\n",
    "        else:\n",
    "            risk_group = 'High Risk'\n",
    "        \n",
    "        return {\n",
    "            'claim_likelihood': claim_likelihood,\n",
    "            'claim_amount': claim_amount,\n",
    "            'normalized_claim_likelihood': normalized_claim_likelihood,\n",
    "            'normalized_claim_amount': normalized_claim_amount,\n",
    "            'risk_profile_probability': risk_profile_probability,\n",
    "            'risk_profile_cost': risk_profile_cost,\n",
    "            'dynamic_combined_risk_score': dynamic_combined_risk_score,\n",
    "            'risk_group': risk_group\n",
    "        }\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'class_model': \"artifacts/model_trainer/class_model.joblib\",\n",
    "    'reg_model': 'artifacts/model_trainer/reg_model.joblib'\n",
    "}\n",
    "scaler_path = 'artifacts/risk_profiles/minmax_scaler.pkl'\n",
    "risk_profile_path = 'artifacts/risk_profiles/risk_profiles.csv'\n",
    "\n",
    "# Initialize the model\n",
    "risk_profile_model = RiskProfileModel(model_paths, scaler_path, risk_profile_path)\n",
    "\n",
    "# Load test data to get columns and dtypes\n",
    "test = pd.read_csv('artifacts/data_transformation/Processed_test_data.csv')\n",
    "columns = test.columns\n",
    "dtypes = test.dtypes\n",
    "\n",
    "# Define user data\n",
    "user_data = {\n",
    "    'gender': 'F',\n",
    "    'agecat': '3',\n",
    "    'credit_score': 670,\n",
    "    'area': 'A',\n",
    "    'traffic_index': 0.1,\n",
    "    'veh_age': '1',\n",
    "    'veh_body': 'SEDAN',\n",
    "    'veh_value': 0.3\n",
    "}\n",
    "\n",
    "# Get predictions\n",
    "claim_likelihood, claim_amount = risk_profile_model.predict(user_data, columns, dtypes)\n",
    "\n",
    "# Normalize predictions\n",
    "normalized_claim_likelihood, normalized_claim_amount = risk_profile_model.normalize_predictions(claim_likelihood, claim_amount)\n",
    "\n",
    "# Classify risk\n",
    "risk_profile = risk_profile_model.classify_risk(normalized_claim_likelihood, normalized_claim_amount, claim_amount)\n",
    "\n",
    "# Output the risk profile\n",
    "print(risk_profile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
